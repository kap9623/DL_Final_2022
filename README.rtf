{\rtf1\ansi\ansicpg1252\cocoartf2636
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Monaco;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;\red199\green200\blue201;
\red0\green0\blue0;\red255\green255\blue255;\red199\green200\blue201;}
{\*\expandedcolortbl;;\cssrgb\c0\c1\c1;\cssrgb\c100000\c100000\c99985\c0;\cssrgb\c82147\c82540\c82727;
\cssrgb\c0\c1\c1;\cssrgb\c100000\c100000\c99985\c0;\cssrgb\c82147\c82540\c82727;}
\margl1440\margr1440\vieww28600\viewh15140\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs26 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4   =====================================================\cf2 \cb3 \strokec4 \
\cf2 \cb3 \strokec4 ||                                                     ||\cf2 \cb3 \strokec4 \
\cf2 \cb3 \strokec4 || Project: Contextualized Medication Event Extraction ||\cf2 \cb3 \strokec4 \
\cf2 \cb3 \strokec4 ||                                                     ||\cf2 \cb3 \strokec4 \
\cf2 \cb3 \strokec4   =====================================================\cf2 \cb3 \strokec4 \
\
\cf2 \cb3 \strokec4 Contributors: Jonathan So, Krutika Pandit\cf2 \cb3 \strokec4 \
\
\cf2 \cb3 \strokec4 Introduction: This project aims to use unstructured data collected  \cf2 \cb3 \strokec4 \
\
\cf2 \cb3 \strokec4 Table of Contents:\cf2 \cb3 \strokec4 \
\cf2 \cb3 \strokec4 1. Data\
    - ann_ref: clinical notes (.txt) and corresponding annotation files (.ann) in the BRAT format\
2. Task 1\cf2 \cb3 \strokec4 \
\cf2 \cb3 \strokec4     - Default Params: testing model performance for note versus patient level inputs (BaseBERT, BioBERT, SciBERT, and ClinicalBERT)\
    - Tuning BaseBERT: assessing effect of weighted loss, freezing embedding layers, various combinations of learning rate and batch size, and early stopping for BaseBERT\
\pard\pardeftab720\partightenfactor0
\cf5 \cb6 \outl0\strokewidth0     - Tuning BioBERT: assessing effect of weighted loss, freezing embedding layers, various combinations of learning rate and batch size, and early stopping for BioBERT\
    - Tuning SciBERT: assessing effect of weighted loss, freezing embedding layers, various combinations of learning rate and batch size, and early stopping for SciBERT\
    - Tuning ClinicalBERT: assessing effect of weighted loss, freezing embedding layers, various combinations of learning rate and batch size, and early stopping for ClinicalBERT\
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \outl0\strokewidth0 \strokec4 \
3. Task 2\cf2 \cb3 \strokec4 \
    - \
\
\
\
\
\cf2 \cb3 \strokec4 Github Repo: https://github.com/kap9623/DL_Final_2022}