  =====================================================
||                                                     ||
|| Project: Contextualized Medication Event Extraction ||
||                                                     ||
  =====================================================

Contributors: Jonathan So, Krutika Pandit

Introduction: 
Unstructured clinical documentation often holds many intricate details regarding medication events that contribute to a comprehensive understanding of a patientâ€™s medication history. 
Using state-of-the-art transformer-based models, we performed named entity recognition and event classification for drug mentions in clinical notes. 
Model performance was assessed after applying a variety of tuning methods including weighted loss, embedding layer freezing, and early stopping. 
Overall, BERT models pre-trained on a domain-specific corpus were among the best performers across both tasks. 

Table of Contents:
1. Data
    - ann_ref: clinical notes (.txt) and corresponding annotation files (.ann) in the BRAT format for Drug labels
    - ann_ref_disp: clinical notes (.txt) and corresponding annotation files (.ann) in the BRAT format for Disposition labels
2. Task 1
    - Default Params: testing model performance for note versus patient level inputs (BaseBERT, BioBERT, SciBERT, and ClinicalBERT)
    - Tuning BaseBERT: assessing effect of weighted loss, freezing embedding layers, various combinations of learning rate and batch size, and early stopping for BaseBERT
    - Tuning BioBERT: assessing effect of weighted loss, freezing embedding layers, various combinations of learning rate and batch size, and early stopping for BioBERT
    - Tuning SciBERT: assessing effect of weighted loss, freezing embedding layers, various combinations of learning rate and batch size, and early stopping for SciBERT
    - Tuning ClinicalBERT: assessing effect of weighted loss, freezing embedding layers, various combinations of learning rate and batch size, and early stopping for ClinicalBERT

3. Task 2
    - Default Parameters: testing model performance with baseline hyperparameters
    - Class Imbablance: testing model performance with class weights implemented
    - Embeddings Freezing: testing model performance with frozen BERT embedding layer
    - ClinicalBERT Hyperparameter Tuning: tuning hyperparameters of clinicalBERT model with class weights
    - ClinicalBERT Epochs + Overfitting: tuning number of epochs of clinicalBERT model with class weights and techniques to reduce overfitting

4. Deep Learning Final Project Paper

Github Repo: https://github.com/kap9623/DL_Final_2022
